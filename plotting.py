from tensorflow import keras
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.gridspec as gridspec

# Plot incorrect images
# Setting incorrecOnly to True plots only wrong predictions
def plot_images_with_prediction(model, class_names, labels, images, row, column, incorrect_only=False):
    # Get predictions
    predictions_array = model.predict(images)

    # Plot grid of images
    num_row = row
    num_column = column
    num_plotted_images = num_row * num_column
    plt.figure(figsize=(2 * 2 * num_column, 2 * num_row))

    num_total_images = len(labels)
    MNISTnumber = -1

    # Plot incorrect predictions with error bar
    j = 0
    for i in range(num_plotted_images):
        if incorrect_only:
            while j < num_total_images - 1 and np.argmax(predictions_array[j]) == labels[j]:
                j += 1
        if j != num_total_images - 1:
            plt.subplot(num_row, 2 * num_column, 2 * i + 1)
            plot_image(j, predictions_array, labels, images,class_names)
            plt.subplot(num_row, 2 * num_column, 2 * i + 2)
            plot_value_array(j, predictions_array, labels, "Predicted Label")
            j += 1
    plt.show()


# Calculate number of wrong images
def get_num_wrong(image_array, label_array,model):
    predictions_array = model.predict(image_array)
    numWrong = 0
    numImages = label_array.shape[0]
    for i in range(numImages):
        if np.argmax(predictions_array[i]) != label_array[i]:
            numWrong += 1

    return numWrong, numWrong / numImages * 100


    # print('numWrong ' + str(numWrong))
    # print('wrong ' + str(numWrong / numImages * 100) + '%')


# Calculate number of predictions that have 100 percent confidence
def get_num_100_predictions(predictions_array):
    length = predictions_array.shape[0]
    num100Confident = 0
    for i in range(length):
        if predictions_array[i][np.argmax(predictions_array[i])] == 1:
            num100Confident += 1
    return num100Confident


# Plot image
def plot_image(i, predictions_array, true_label, img, class_names):
    predictions_array, true_label, img = predictions_array[i], true_label[i], img[i]
    plt.grid(False)
    plt.xticks([])
    plt.yticks([])

    plt.imshow(img, cmap=plt.cm.binary)

    predicted_label = np.argmax(predictions_array)
    if predicted_label == true_label:
        color = 'blue'
    else:
        color = 'red'

    plt.xlabel("Image# {} \n Predicted {} ({}) \n {:2.1f}% confidence".format(i,class_names[predicted_label], class_names[true_label],
                                                                 100 * np.max(predictions_array)),
               color=color)


# Plot prediction distribution
def plot_value_array(i, predictions_array, true_label, title=''):
    predictions_array, true_label = predictions_array[i], true_label[i]
    plt.grid(False)
    plt.xticks(np.arange(0, 10, step=1))
    # plt.yticks(np.arange(0, 2, step=1))
    thisplot = plt.bar(range(10), predictions_array, color="#777777")
    plt.ylim([0, 1])
    predicted_label = np.argmax(predictions_array)
    thisplot[predicted_label].set_color('red')
    thisplot[true_label].set_color('blue')
    plt.title(title)


# Plot prediction distribution with ground truth distribution
def plot_with_ground_truth(predictions, labels, images, row):
    one_hot_encodings = keras.utils.to_categorical(labels)

    for i in range(row):
        plt.subplot(row, 3, 3 * i + 1)
        plot_image(i, predictions, labels, images)

        plt.subplot(row, 3, 3 * i + 2)
        plot_value_array(i, predictions, labels, "Predicted Label")

        plt.subplot(row, 3, 3 * i + 3)
        plot_value_array(i, one_hot_encodings, labels, "Ground Truth")

    plt.show()


# Plot anything
def plot(input1, l1='', input2=None, l2='', input3=None, l3='', input4=None, l4='', input5=None, l5='',input6=None, l6='',input7=None, l7=''):

    epochs = len(input1)
    dim = np.arange(1, epochs + 1)
    plt.plot(dim, input1, 'o--', label= l1)
    if input2:
        plt.plot(dim, input2, 'o--', label=l2)
    if input3:
        plt.plot(dim, input3, 'o--', label=l3)
    if input4:
        plt.plot(dim, input4, 'o--', label=l4)
    if input5:
        plt.plot(dim, input5, 'o--', label=l5)
    if input6:
        plt.plot(dim, input6, 'o--', label=l6)
    if input7:
        plt.plot(dim, input7, 'o--', label=l7)

    #plt.xticks(dim)
    plt.title('Adam Optimizer')
    plt.legend(loc='best')
    plt.xlabel("Epoch")
    plt.ylabel("Accuracy")
    plt.show()


def plot_4_history(train_loss, train_acc, val_loss, val_acc, description, start=0, end=None):

    # How much too plot, everything by default
    if not end:
        end = len(train_loss)
    train_loss = train_loss[start:end]
    train_acc = train_acc[start:end]
    val_loss = val_loss[start:end]
    val_acc = val_acc[start:end]

    # Necessary so that first epoch is plotted at 1 instead of 0
    dim = np.arange(start + 1, end + 1)

    # Change marker based on amount of data
    marker = 'o--'
    if end - start > 50:
        marker = '--'

    # GridSpec is necessary to prevent suptitle from overlapping
    fig = plt.figure(1)
    gs1 = gridspec.GridSpec(1, 2)
    ax_list = [fig.add_subplot(ss) for ss in gs1]

    # Loss plot on left
    ax_list[0].plot(dim, train_loss, marker, label="Training Set")
    ax_list[0].plot(dim, val_loss, marker, label="Test Set")
    ax_list[0].set_xlabel("Epoch")
    ax_list[0].set_ylabel("Loss")
    ax_list[0].legend(loc='best')
    ax_list[0].set_title('Cross-Entropy Loss', fontsize=12)

    # Accuracy plot on right
    ax_list[1].plot(dim, train_acc, marker, label="Training Set")
    ax_list[1].plot(dim, val_acc, marker, label="Test Set")
    ax_list[1].set_xlabel("Epoch")
    ax_list[1].set_ylabel("Accuracy")
    ax_list[1].set_title('Accuracy', fontsize=12)
    ax_list[1].legend(loc='best')

    # Add title
    fig.suptitle(description, fontsize=14)
    gs1.tight_layout(fig, rect=[0, 0.03, 1, 0.95])
    plt.show()

# Plot one thing
def plot_one_thing(data,xlabel="",ylabel="",title="",legend_label= ""):

    epoch = len(data)
    dim = np.arange(1, epoch + 1)
    marker = "*--"
    if len(data) >  1:
        marker = "*"
    plt.plot(dim, data, marker, label=legend_label)
    plt.legend(loc='best')
    plt.xticks(dim)
    plt.xlabel(xlabel)
    plt.ylabel(ylabel)
    plt.title(title)
    plt.show()


loss=[0.9195748946070671, 0.42985786964495976, 0.34252202689647676, 0.29764544664571685, 0.268050238614281, 0.24967645774285, 0.23730851422995328, 0.22795838821679354, 0.22090781020621458, 0.21593410020073256, 0.2100073729828, 0.2056276524811983, 0.20267911836504937, 0.1989025669420759, 0.19678417202085258, 0.19394297059004506, 0.19172258858258526, 0.1887526184817155, 0.18706122521311044, 0.18403424258654316, 0.1832299621279041, 0.18187286322315535, 0.17965119655554493, 0.17805008828019103, 0.17694276373522977, 0.17623546735073128, 0.17504878940060734, 0.17419285867363216, 0.17174352293213208, 0.17087557590877017, 0.1703952472532789, 0.16925387783596912, 0.16889735639095307, 0.16734703594818712, 0.16697201337044437, 0.16560141973818343, 0.1654989899508655, 0.16475498402491212, 0.16353741979847353, 0.16314140907799204, 0.1622361410781741, 0.16106038959076008, 0.16085747037703793, 0.16003181255112092, 0.16008003380149602, 0.15825518870105346, 0.15846195860765874, 0.15724033906434973, 0.15727360574218133, 0.15621300171439848, 0.1558098106769224, 0.15556414909971258, 0.1547110874267916, 0.15511374559874336, 0.1538575942379733, 0.15320652942794064, 0.15241491072190305, 0.15243764786670605, 0.15230822626190882, 0.1518850363108019, 0.15082410788784426, 0.1506410348104934, 0.15111329789894323, 0.1501661207837363, 0.14965895333637794, 0.1485186975946029, 0.14923651113795738, 0.14857018483181794, 0.14795877480879427, 0.1474729306468119, 0.14760514412075282, 0.14696021201709905, 0.146402429662024, 0.14614120941919584, 0.1458083967740337, 0.1453040530035893, 0.14561499365915856, 0.14500734354369343, 0.14467873806444306, 0.14435970682340363, 0.1438233508169651, 0.1441349844262004, 0.14393289314893384, 0.1433858801238239, 0.14296697694808244, 0.14296932087590297, 0.14254534079382816, 0.14291445745155215, 0.14139457978929082, 0.1414701864744226, 0.14133358830275636, 0.14149789049911002, 0.14085920209685962, 0.14143101221260926, 0.14107586858794094, 0.14004684407884876, 0.13978028223849834, 0.13941103764809668, 0.13963713499717414, 0.1397033427702263, 0.13930658250736694, 0.13870311981377503, 0.13867695847526193, 0.13879663314670324, 0.13805716161616147, 0.13809315618438026, 0.1380399227576951, 0.13806548923874895, 0.1377877000843485, 0.1377407859979818, 0.13672160113230347, 0.13707493740754823, 0.1365520202430586, 0.13654339268182714, 0.13611048502537113, 0.13629662192426623, 0.13683689877390862, 0.13522822419491906, 0.13589216665985684, 0.13545740585463742, 0.13471430986809232, 0.13476020760213336, 0.13496335547727842, 0.1346956759194533, 0.1357354368455708, 0.13426339189211528, 0.1344607827688257, 0.13381420324246088, 0.13364551164520283, 0.1336961005317668, 0.13420021923879782, 0.1334728174082314, 0.13376518099258344, 0.13281597894616426, 0.1330646006949246, 0.13350793475906053, 0.13302073516572516, 0.1322737347551932, 0.1326511563360691, 0.13207228401054938, 0.13261729925560453, 0.13180416227318348, 0.13193091002603372, 0.13145976956312855, 0.13192019903411467, 0.13085424020886421, 0.13125798146240414, 0.13107308092527092, 0.1309977035348614, 0.13099956034993132, 0.1315965997024129, 0.1306456461766114, 0.13124310979619622, 0.12997695808609327, 0.1300776432796071, 0.1310485288935403, 0.12950604350926975, 0.1297187187212209, 0.12939613473601638, 0.12929186567043266, 0.12964876536279918, 0.1297408286575228, 0.1288433925093462, 0.12908008007643124, 0.1287255057816704, 0.12886642701302967, 0.1281903123979767, 0.12900662817060948, 0.1286677152849734, 0.12842964531543355, 0.12774584600701927, 0.12818764188637335, 0.12760824501048773, 0.12763960228922466, 0.12714386801235378, 0.12794910765253006, 0.1285558704317858, 0.12703017595534524, 0.12725445851062736, 0.12685322069873411, 0.12711901418243846, 0.1272337583359331, 0.12764470431332786, 0.12647285110006729, 0.12759561246261, 0.12728607684994736, 0.126470680963248, 0.12603485254570843, 0.12661130083724856, 0.1266913093160838, 0.12638677258665362, 0.12605359612343212, 0.1255083197231094, 0.12597290624243518, 0.12549984239662687, 0.1254867183882743, 0.12544226438427966, 0.12451548009179532, 0.12532298696537814, 0.12504135690939924]
acc=[0.7057833, 0.87521666, 0.9019, 0.91506666, 0.9241833, 0.92845, 0.93198335, 0.93465, 0.93686664, 0.93798333, 0.93958336, 0.9410333, 0.9411333, 0.9433333, 0.94261664, 0.9439667, 0.9445, 0.9454833, 0.94593334, 0.94715, 0.94671667, 0.9475667, 0.9486333, 0.94845, 0.94915, 0.9494, 0.9496667, 0.9498, 0.95015, 0.95065, 0.9507167, 0.9507833, 0.95065, 0.9522833, 0.9507833, 0.95173335, 0.95211667, 0.9528, 0.95315, 0.9526167, 0.9529833, 0.95301664, 0.95288336, 0.9536833, 0.95355, 0.95381665, 0.9536, 0.95448333, 0.95435, 0.95453334, 0.95493335, 0.9549, 0.9548, 0.95435, 0.95521665, 0.955, 0.95601666, 0.9551167, 0.95526665, 0.95555, 0.95615, 0.95556664, 0.9551, 0.95566666, 0.9558333, 0.95725, 0.9561, 0.9566, 0.95655, 0.9564667, 0.9569167, 0.95701665, 0.9567, 0.95685, 0.95746666, 0.9576167, 0.95775, 0.9576, 0.95811665, 0.9577, 0.9574, 0.95731664, 0.9579333, 0.9581, 0.95771664, 0.95735, 0.9582667, 0.95808333, 0.95811665, 0.9587167, 0.9587, 0.9582, 0.9583333, 0.9583333, 0.9582667, 0.95905, 0.95876664, 0.95876664, 0.959, 0.9586333, 0.95926666, 0.95881665, 0.95951664, 0.95876664, 0.95911664, 0.95958334, 0.95951664, 0.9593167, 0.9590167, 0.95911664, 0.95925, 0.95956665, 0.95958334, 0.9597833, 0.95956665, 0.9598167, 0.95945, 0.9602, 0.9598, 0.96005, 0.96045, 0.95985, 0.9601833, 0.96028334, 0.9600833, 0.9605, 0.9600833, 0.96015, 0.9606, 0.96098334, 0.95985, 0.96026665, 0.9604667, 0.96033335, 0.9607667, 0.96035, 0.9607667, 0.9608, 0.96071666, 0.96068335, 0.96061665, 0.96068335, 0.96063334, 0.9613, 0.96071666, 0.96145, 0.9612833, 0.96136665, 0.96135, 0.96148336, 0.9608167, 0.96175, 0.96068335, 0.96175, 0.9612833, 0.96136665, 0.96136665, 0.96166664, 0.9619, 0.9612667, 0.9615167, 0.96141666, 0.96208334, 0.96165, 0.9618667, 0.9619333, 0.9619667, 0.9615667, 0.96225, 0.96165, 0.96243334, 0.9615, 0.9619333, 0.9623, 0.96275, 0.96183336, 0.96213335, 0.96275, 0.96208334, 0.96245, 0.96203333, 0.9618667, 0.96173334, 0.9626, 0.96203333, 0.96265, 0.9623333, 0.9626167, 0.9623333, 0.96216667, 0.96245, 0.96265, 0.9630167, 0.9619167, 0.96245, 0.96241665, 0.9619, 0.96325, 0.96271664, 0.96281666]
val_loss=[0.4917675682902336, 0.3559177501499653, 0.31069821417331694, 0.2744959033280611, 0.25558515653014185, 0.24511561363935472, 0.23439102716743945, 0.23374234579503536, 0.23143061198294163, 0.2212984162569046, 0.2217339564859867, 0.2208475098013878, 0.22378724865615368, 0.22245060510933398, 0.21802250631153583, 0.2233296437561512, 0.216444933116436, 0.21760275073349475, 0.22209301207214593, 0.21731345672160388, 0.22211723797023297, 0.21952867299318313, 0.22202255435287951, 0.22171397715806962, 0.2147060377523303, 0.21876871842890977, 0.21994694299995898, 0.21747518196702004, 0.2187462482601404, 0.2223029462248087, 0.21970581591129304, 0.2159788381308317, 0.2164877676218748, 0.21990265913307666, 0.22463447995483876, 0.2230110615119338, 0.219536609351635, 0.21845280300825834, 0.22554798662662506, 0.21781018417328596, 0.21811592973768712, 0.2188258257135749, 0.22068574294447899, 0.22573390893638134, 0.21840792387723923, 0.22295334991067647, 0.2206106870621443, 0.22745860122144224, 0.21992063857614993, 0.22091225013136864, 0.22892681203782558, 0.21764016196131705, 0.22027126725763083, 0.22530014321208, 0.22308851134032012, 0.2196721414476633, 0.2195043847709894, 0.22387508556246757, 0.2227452166378498, 0.220501209422946, 0.21760391920804978, 0.2232991324365139, 0.22117243595421315, 0.2264948896318674, 0.220332240909338, 0.22586427371948958, 0.2222060925886035, 0.22297784872353077, 0.2324427580833435, 0.21820412389934063, 0.22369590293616057, 0.2219457171484828, 0.22317180041223764, 0.23831627137959002, 0.22254912573844193, 0.22432254672050475, 0.22147637739777565, 0.22271645430475473, 0.224276752024889, 0.21949137534946203, 0.22575044825673105, 0.2233395541459322, 0.2215071764588356, 0.2202998481504619, 0.22266773955896496, 0.22961744029074907, 0.22924559157341717, 0.22352683395147324, 0.2232292740419507, 0.22706513833254577, 0.22085002049803734, 0.22329961657524108, 0.22559946943074466, 0.22969224821776152, 0.2227580900862813, 0.22669511348009108, 0.2284942890703678, 0.22548753447830677, 0.2298565264046192, 0.22480455983430148, 0.23213592160493135, 0.2295326914638281, 0.22879513036459684, 0.23145900163799524, 0.23063590865582229, 0.22904120914638043, 0.22669232334941625, 0.22651282876729964, 0.22609837159514426, 0.2245164468511939, 0.23289284866303206, 0.22673268776386976, 0.22982012726366519, 0.23206094391644, 0.2314680526033044, 0.22786291845142842, 0.22832786630839108, 0.23266316033899784, 0.22611579965800047, 0.2379794692993164, 0.23096445094794035, 0.22985252719372512, 0.23592440029606224, 0.2351723952591419, 0.23358967296779157, 0.23692826684564353, 0.23474552016705275, 0.23363190930336714, 0.23250437952578068, 0.23356163565069438, 0.23386512987315655, 0.23312763310968876, 0.23350425373762845, 0.23618607100099326, 0.23566860239952803, 0.23409090336412192, 0.23536245357245206, 0.23305806193500758, 0.23155739430338143, 0.22820965722203254, 0.2345713195949793, 0.23169778056442739, 0.23132969543337822, 0.23415815100073814, 0.23088980332016945, 0.23055422641336917, 0.23238264463841915, 0.2390633210539818, 0.2321653299406171, 0.23316573962569237, 0.23400119718164206, 0.2354752205684781, 0.2339511887729168, 0.23623822219669818, 0.23748005513101816, 0.2379750919342041, 0.23758465092629194, 0.24120703678578137, 0.24068158581852914, 0.24268330305814742, 0.2410038410127163, 0.23514688432216643, 0.2349734890833497, 0.237035101801157, 0.23630825765430927, 0.24341959649696945, 0.23636337473988533, 0.2387934487313032, 0.23860500585287808, 0.24094967506825923, 0.23517340030521156, 0.23903343468904495, 0.24414767801761628, 0.24049556359648705, 0.23929056912660598, 0.2400113543495536, 0.23680796612054109, 0.23735091604292394, 0.23809548802673816, 0.2383220472186804, 0.2409956967830658, 0.23963505089282988, 0.24423397552222015, 0.2481989449262619, 0.2395478992909193, 0.23931626308709383, 0.239858841560781, 0.23873656906187535, 0.2412876754999161, 0.2426627766340971, 0.24002338904887438, 0.24585702635347842, 0.2465553393959999, 0.24374807838350535, 0.2424765370413661, 0.23876776922494172, 0.24392651800066234, 0.24628351360559464, 0.24941711612045764, 0.25552765134721994]
val_acc=[0.8498, 0.8939, 0.9078, 0.9185, 0.9249, 0.9299, 0.9313, 0.9358, 0.9355, 0.9371, 0.937, 0.9355, 0.9363, 0.9347, 0.9382, 0.938, 0.938, 0.9382, 0.9376, 0.9381, 0.9364, 0.9399, 0.9367, 0.9365, 0.9391, 0.9393, 0.9393, 0.9385, 0.9394, 0.9364, 0.9385, 0.9403, 0.9393, 0.9386, 0.9376, 0.9382, 0.9389, 0.9404, 0.9389, 0.941, 0.9398, 0.9392, 0.9404, 0.9397, 0.9407, 0.9375, 0.94, 0.9371, 0.938, 0.9392, 0.9384, 0.9403, 0.9398, 0.9401, 0.94, 0.9381, 0.9401, 0.9384, 0.9389, 0.9401, 0.9398, 0.9387, 0.9394, 0.9377, 0.9406, 0.9376, 0.9385, 0.942, 0.9362, 0.9408, 0.9384, 0.9391, 0.9404, 0.9359, 0.9411, 0.9405, 0.9394, 0.9396, 0.9404, 0.9407, 0.9385, 0.9388, 0.941, 0.9401, 0.9399, 0.9378, 0.9371, 0.9407, 0.9391, 0.938, 0.9389, 0.9392, 0.9396, 0.9402, 0.9406, 0.9385, 0.9386, 0.94, 0.9407, 0.9414, 0.9368, 0.9394, 0.9393, 0.9398, 0.94, 0.9389, 0.9393, 0.9398, 0.9403, 0.9405, 0.9376, 0.9416, 0.9406, 0.9404, 0.9391, 0.9384, 0.9406, 0.9379, 0.9415, 0.9395, 0.9416, 0.9419, 0.9384, 0.9386, 0.9388, 0.9393, 0.9414, 0.9399, 0.9382, 0.9386, 0.939, 0.9408, 0.9397, 0.9408, 0.9411, 0.9405, 0.9395, 0.9419, 0.9391, 0.9396, 0.94, 0.9413, 0.9413, 0.9397, 0.9419, 0.9402, 0.9403, 0.9391, 0.9405, 0.941, 0.94, 0.9402, 0.9405, 0.939, 0.9397, 0.9398, 0.9403, 0.9386, 0.9392, 0.9415, 0.9398, 0.9384, 0.9411, 0.9404, 0.9385, 0.9401, 0.9404, 0.9393, 0.9396, 0.9412, 0.94, 0.9403, 0.9376, 0.9394, 0.9395, 0.939, 0.941, 0.941, 0.94, 0.9377, 0.9402, 0.9403, 0.9368, 0.9381, 0.9388, 0.9411, 0.9409, 0.9414, 0.941, 0.9394, 0.9392, 0.9375, 0.9381, 0.9408, 0.9396, 0.9403, 0.9397, 0.9395, 0.9376, 0.9357]

#plot_4_history(loss,acc,val_loss,val_acc,"784-10-10-10 Network")
#plot_one_thing(lr[:10],"Epoch","Learning Rate","Step Decay Learning Rate")



# plot(adam_train_acc)
# plot(adam_097decayLoss)

# plot(adam_097decayLoss,'loss',adam_097decayAccuracy,'accuracy')


#plot(tdecay001,'0.001 decay every iteration',nodecay_50,'no decay')

#
# plt.plot(dim,[0.9168, 0.9274, 0.9339, 0.9399, 0.9439, 0.9465, 0.9485, 0.9499, 0.9509, 0.9469], 'bo--')
# plt.plot(dim,[0.9184, 0.9302, 0.9378, 0.9443, 0.9468, 0.9466, 0.9491, 0.9504, 0.9532, 0.9535], 'bo--',label='Scaling')
# plt.xticks(dim)
# plt.title('Effect of Scaling')
# plt.legend(loc='lower right')
# plt.xlabel("Epoch")
# plt.ylabel("Accuracy")
# plt.show()
#
# t = np.arange(0., 11)


