import tensorflow as tf
from tensorflow import keras
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.gridspec as gridspec

# Plot incorrect images
# Setting incorrecOnly to True plots only wrong predictions
def plot_images_with_prediction(predictions_array, labels, images, row, column, incorrect_only=False):
    # Plot grid of images
    num_row = row
    num_column = column
    num_plotted_images = num_row * num_column
    plt.figure(figsize=(2 * 2 * num_column, 2 * num_row))

    num_total_images = len(labels)

    # Plot incorrect predictions with error bar
    j = 0
    for i in range(num_plotted_images):
        if incorrect_only:
            while j < num_total_images - 1 and np.argmax(predictions_array[j]) == labels[j]:
                j += 1
        if j != num_total_images - 1:
            plt.subplot(num_row, 2 * num_column, 2 * i + 1)
            plot_image(j, predictions_array, labels, images)
            plt.subplot(num_row, 2 * num_column, 2 * i + 2)
            plot_value_array(j, predictions_array, labels, "Predicted Label")
            j += 1
    plt.show()


# Calculate number of wrong images
def get_num_wrong(image_array, label_array,model):
    predictions_array = model.predict(image_array)
    numWrong = 0
    numImages = label_array.shape[0]
    for i in range(numImages):
        if np.argmax(predictions_array[i]) != label_array[i]:
            numWrong += 1

    return numWrong, numWrong / numImages * 100


    # print('numWrong ' + str(numWrong))
    # print('wrong ' + str(numWrong / numImages * 100) + '%')


# Calculate number of predictions that have 100 percent confidence
def get_num_100_predictions(predictions_array):
    length = predictions_array.shape[0]
    num100Confident = 0
    for i in range(length):
        if predictions_array[i][np.argmax(predictions_array[i])] == 1:
            num100Confident += 1
    return num100Confident


# Plot image
def plot_image(i, predictions_array, true_label, img, class_names):
    predictions_array, true_label, img = predictions_array[i], true_label[i], img[i]
    plt.grid(False)
    plt.xticks([])
    plt.yticks([])

    plt.imshow(img, cmap=plt.cm.binary)

    predicted_label = np.argmax(predictions_array)
    if predicted_label == true_label:
        color = 'blue'
    else:
        color = 'red'

    plt.xlabel("Predicted {} ({}) \n {:2.1f}% confidence".format(class_names[predicted_label], class_names[true_label],
                                                                 100 * np.max(predictions_array)),
               color=color)


# Plot prediction distribution
def plot_value_array(i, predictions_array, true_label, title=''):
    predictions_array, true_label = predictions_array[i], true_label[i]
    plt.grid(False)
    plt.xticks(np.arange(0, 10, step=1))
    # plt.yticks(np.arange(0, 2, step=1))
    thisplot = plt.bar(range(10), predictions_array, color="#777777")
    plt.ylim([0, 1])
    predicted_label = np.argmax(predictions_array)
    thisplot[predicted_label].set_color('red')
    thisplot[true_label].set_color('blue')
    plt.title(title)


# Plot prediction distribution with ground truth distribution
def plot_with_ground_truth(predictions, labels, images, row):
    one_hot_encodings = keras.utils.to_categorical(labels)

    for i in range(row):
        plt.subplot(row, 3, 3 * i + 1)
        plot_image(i, predictions, labels, images)

        plt.subplot(row, 3, 3 * i + 2)
        plot_value_array(i, predictions, labels, "Predicted Label")

        plt.subplot(row, 3, 3 * i + 3)
        plot_value_array(i, one_hot_encodings, labels, "Ground Truth")

    plt.show()


# Plot anything
def plot(input1, l1='', input2=None, l2='', input3=None, l3='', input4=None, l4='', input5=None, l5='',input6=None, l6='',input7=None, l7=''):

    epochs = len(input1)
    dim = np.arange(1, epochs + 1)
    plt.plot(dim, input1, 'o--', label= l1)
    if input2:
        plt.plot(dim, input2, 'o--', label=l2)
    if input3:
        plt.plot(dim, input3, 'o--', label=l3)
    if input4:
        plt.plot(dim, input4, 'o--', label=l4)
    if input5:
        plt.plot(dim, input5, 'o--', label=l5)
    if input6:
        plt.plot(dim, input6, 'o--', label=l6)
    if input7:
        plt.plot(dim, input7, 'o--', label=l7)

    #plt.xticks(dim)
    plt.title('Adam Optimizer')
    plt.legend(loc='best')
    plt.xlabel("Epoch")
    plt.ylabel("Accuracy")
    plt.show()


def plot_4_history(train_loss, train_acc, val_loss, val_acc, start=0, end=None):

    # How much too plot, everything by default
    if not end:
        end = len(train_loss)
    train_loss = train_loss[start:end]
    train_acc = train_acc[start:end]
    val_loss = val_loss[start:end]
    val_acc = val_acc[start:end]

    # Necessary so that first epoch is plotted at 1 instead of 0
    dim = np.arange(start + 1, end + 1)

    # Change marker based on amount of data
    marker = 'o--'
    if end - start > 50:
        marker = '--'

    # GridSpec is necessary to prevent suptitle from overlapping
    fig = plt.figure(1)
    gs1 = gridspec.GridSpec(1, 2)
    ax_list = [fig.add_subplot(ss) for ss in gs1]

    # Loss plot on left
    ax_list[0].plot(dim, train_loss, marker, label="Training Set")
    ax_list[0].plot(dim, val_loss, marker, label="Test Set")
    ax_list[0].set_xlabel("Epoch")
    ax_list[0].set_ylabel("Loss")
    ax_list[0].legend(loc='best')
    ax_list[0].set_title('Cross-Entropy Loss', fontsize=12)

    # Accuracy plot on right
    ax_list[1].plot(dim, train_acc, marker, label="Training Set")
    ax_list[1].plot(dim, val_acc, marker, label="Test Set")
    ax_list[1].set_xlabel("Epoch")
    ax_list[1].set_ylabel("Accuracy")
    ax_list[1].set_title('Accuracy', fontsize=12)
    ax_list[1].legend(loc='best')

    # Add title
    fig.suptitle('784-1024-1024-10 Network', fontsize=14)
    gs1.tight_layout(fig, rect=[0, 0.03, 1, 0.95])
    plt.show()

# Plot one thing
def plot_one_thing(data,xlabel="",ylabel="",title="",legend_label= ""):

    epoch = len(data)
    dim = np.arange(1, epoch + 1)
    marker = "*--"
    if len(data) >  1:
        marker = "*"
    plt.plot(dim, data, marker, label=legend_label)
    plt.legend(loc='best')
    plt.xticks(dim)
    plt.xlabel(xlabel)
    plt.ylabel(ylabel)
    plt.title(title)
    plt.show()


loss = [0.3648515035646657, 0.14683437823628387, 0.10146191086309651, 0.07662140380591154, 0.05914511546337356, 0.046871367919569216, 0.03644665260023127, 0.02932750750759927, 0.023234541584194326, 0.018097344374400564, 0.014286069678491912, 0.011352773717371747, 0.009301175975609416, 0.006751189521940736, 0.0051844375155390785, 0.004162137150027168, 0.00344761789686648, 0.0034726277657803925, 0.0025390086331026395, 0.0013800595316812783, 0.0011971083162158418, 0.0008449062251505286, 0.003846622500926363, 0.002211560890694576, 0.0006178752041311479, 0.00042440307583092363, 0.00032026493881251856, 0.0003439228067342507, 0.004039851297341859, 0.0003907264380965595, 0.00025475318874365865, 0.00020934172095924926, 0.00017553969388851934, 0.00015226056736689013, 0.0001321346276533101, 0.0001115764455456277, 0.0033694028096897454, 0.0004219872451691723, 0.00017443735219785595, 0.0001340776962357874, 0.00010898364744813686, 9.205254893079958e-05, 7.811115069065029e-05, 6.695452764878004e-05, 6.54156257807396e-05, 5.2190063666633556e-05, 4.3947635662486086e-05, 0.0023004745572734464, 0.0006656353895292947, 0.00010136950621093395, 6.917144631605272e-05, 5.789854490065712e-05, 5.001652692802357e-05, 4.344050996716457e-05, 3.808002006583896e-05, 3.334717612536527e-05, 2.899166159939644e-05, 2.5553411072678502e-05, 2.2207878513427203e-05, 1.977920731633276e-05, 1.7236903485127186e-05, 1.4906343427583124e-05, 1.3022656125940558e-05, 0.0009676631145085443, 0.00010264426811848656, 2.8690862221765202e-05, 2.2607293171859055e-05, 1.9357628944286867e-05, 1.6969670503878358e-05, 1.5124066070484332e-05, 1.3458205778154782e-05, 1.2165456260314992e-05, 1.0890363849966889e-05, 9.836034906243185e-06, 8.862572350949449e-06, 8.023362634238158e-06, 7.17328363142163e-06, 6.526472337782252e-06, 5.755429106670817e-06, 5.210537748136327e-06, 4.657914400543935e-06, 4.183391337543677e-06, 3.8788944209500185e-06, 3.462427056509417e-06, 3.1438510291081913e-06, 0.00017380580952855478, 1.644363076574488e-05, 3.894767399591122e-06, 3.3355764973445426e-06, 3.057209980852349e-06, 2.851444197062847e-06, 2.6781883123779457e-06, 2.528530895347103e-06, 2.3943071916695165e-06, 2.2685937009706928e-06, 2.1508814448149376e-06, 2.054368160694745e-06, 1.955034492387616e-06, 1.8748583530244408e-06, 1.7915075363816867e-06, 1.7177672051123712e-06, 1.6549993949865894e-06, 1.5895266507944446e-06, 1.5338354673607075e-06, 1.4874287404609276e-06, 1.437803151513132e-06, 1.3985363240938871e-06, 1.3629888822682308e-06, 1.3254467836532058e-06, 1.2964832821656575e-06, 1.2681992899388207e-06, 1.243131989099311e-06, 1.2212871666861247e-06, 1.2007397269068558e-06, 1.1813505293882069e-06, 1.1674309949209298e-06, 1.151588307379825e-06, 1.141602564113479e-06, 1.1281340435213375e-06, 1.1186967331392832e-06, 1.1075944613973359e-06, 1.0991803425971132e-06, 1.0929695529663755e-06, 1.0853084348620238e-06, 1.0791850706937111e-06, 1.0738624231976246e-06, 1.06801721813099e-06, 1.0635151089862424e-06, 1.0593507522571599e-06, 1.0549757951139327e-06, 1.051147215018015e-06, 1.0482444739068342e-06, 1.044554960382508e-06, 1.0416899825334743e-06, 1.0388488374246662e-06, 1.036353400062732e-06, 1.0340149157173073e-06, 1.0314896780035571e-06, 1.029453194026549e-06, 1.0275816157445661e-06, 1.025537178804825e-06, 1.0238046786525956e-06, 1.0218019716035088e-06, 1.0203555691153573e-06, 1.0187899596303397e-06, 1.0173316353719505e-06, 1.0160521249720962e-06, 1.0146514241190138e-06, 1.0134792020721762e-06, 1.0121738639175721e-06, 1.011085087156971e-06, 1.0100579096198695e-06, 1.008967145897562e-06, 1.0078843326747725e-06, 1.0070399326878033e-06, 1.0061518271224183e-06, 1.005243850424146e-06, 1.004397468212422e-06, 1.0036027445418464e-06, 1.0029570238619575e-06, 1.0022000512321937e-06, 1.0015424154137994e-06, 1.0007556365584e-06, 1.0001973409619798e-06, 9.994721546983479e-07, 9.98868160403769e-07, 9.98260194080558e-07, 9.977694463714215e-07, 9.972469191173634e-07, 9.96698558897909e-07, 9.962197358011811e-07, 9.956713739711632e-07, 9.952243379037402e-07, 9.947037938218272e-07, 9.94330275905971e-07, 9.93952777719187e-07, 9.934918386989012e-07, 9.931044060825418e-07, 9.92629561172483e-07, 9.922937878551844e-07, 9.91979870927177e-07, 9.91614298773887e-07, 9.911911040679418e-07, 9.908295088697135e-07, 9.906268495759227e-07, 9.902692221430698e-07, 9.899612617421855e-07, 9.897089389217703e-07, 9.893175390137306e-07, 9.8912283097737e-07, 9.888128890149044e-07, 9.88536718201279e-07, 9.882824070928108e-07, 9.880360425048215e-07, 9.878274253575607e-07, 9.875254271681418e-07, 9.872929753858746e-07, 9.870585308628203e-07, 9.86746600612302e-07, 9.866313644124602e-07]
acc = [0.906, 0.95753336, 0.97078335, 0.9781833, 0.98365, 0.9866167, 0.99023336, 0.99191666, 0.99403334, 0.99595, 0.9971333, 0.9978667, 0.9981167, 0.999, 0.9993167, 0.99955, 0.9996, 0.9995667, 0.9996333, 0.9999833, 0.99995, 0.9999667, 0.9989667, 0.9996667, 0.9999833, 1.0, 1.0, 0.9999833, 0.9988667, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.999, 0.9999833, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9993333, 0.99983335, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9996333, 0.9999833, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99995, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
val_loss = [0.18237555228173732, 0.12583508849143982, 0.09830521991476417, 0.08319133610464632, 0.07583675020374357, 0.06812895712442696, 0.06608701237011701, 0.07013843880034983, 0.06487209929153323, 0.06833048669621349, 0.06113420644309372, 0.07277638207189739, 0.06561225010082125, 0.06979059667792172, 0.06908557954011485, 0.061316441759699955, 0.06648475712863729, 0.06637521588709205, 0.07007682532654144, 0.07019192479201593, 0.06942998668760993, 0.07344046237180009, 0.08339842050918378, 0.07412955388659612, 0.07355012803745922, 0.07438035821047378, 0.07442841154377675, 0.07729035860189469, 0.07894037495789234, 0.0754928176291287, 0.07624639612884493, 0.07780492243589833, 0.0779428274917882, 0.07898217090625621, 0.08066026600121404, 0.08139335261948873, 0.09095880858745659, 0.08399470089527312, 0.08351947752351407, 0.08382757170184049, 0.08407868703623535, 0.08493668162729591, 0.08599591857549967, 0.08629589834599756, 0.08851031158563273, 0.08969289868022315, 0.08860545634751815, 0.10004217895926558, 0.09017168303369544, 0.08845935248082241, 0.08843329850613373, 0.08858215087521785, 0.08894630824725028, 0.08949447810475249, 0.08946959217649236, 0.09064578761295707, 0.09072582627703013, 0.09150663120681429, 0.09208772575482727, 0.09288018213053874, 0.09526875140228185, 0.09479994354092923, 0.0957689538723207, 0.10676547533337725, 0.0995923141100866, 0.09873232307738362, 0.09849073955849462, 0.09834761580896156, 0.09789966166844351, 0.09807241316366344, 0.098039125835935, 0.09838020412977584, 0.0985251576165274, 0.09884576499574906, 0.09914469657625886, 0.09963095607639844, 0.09991539956930864, 0.10080340024504039, 0.10131001364481108, 0.10202802598840208, 0.10280084684726716, 0.10358313480217475, 0.10444843137242969, 0.10480699390216613, 0.10568125860234204, 0.11201292691136587, 0.10864923858745897, 0.10857727585890188, 0.10846953322422906, 0.10852819680393623, 0.10865014510563924, 0.10887579410971966, 0.10858898221107666, 0.10915035036096014, 0.10887744264715707, 0.10947870003719118, 0.109135994219032, 0.10997520630696954, 0.11024471349767509, 0.1104098997105757, 0.11140740049308079, 0.11123594162502741, 0.11153433933199267, 0.11291616802052885, 0.11281296652375204, 0.11324569851988599, 0.11348089195146258, 0.1137348863468651, 0.11402336205697793, 0.1147497030969521, 0.11473170623228611, 0.1146126806994198, 0.11509029630875148, 0.11544065324403846, 0.115790004437049, 0.1160943775896817, 0.11677261685548729, 0.11664688114702017, 0.11738886416796959, 0.11732537993965365, 0.1176372127226125, 0.11785987949982882, 0.1180221501925007, 0.11831659152847805, 0.11825713903057476, 0.11869940570658401, 0.11854105076050019, 0.11868473897669218, 0.11904650978255632, 0.11926841256938132, 0.11916907345573463, 0.11909904441029312, 0.11937553698003171, 0.11945230426004855, 0.11982199347469986, 0.11998449816098741, 0.12000484511048512, 0.1200381430910511, 0.12016984575729112, 0.12066507185303635, 0.12056873263717421, 0.12067454092471508, 0.12061243547769891, 0.12089071655098906, 0.12073322645614326, 0.12103306455355778, 0.12098998589023723, 0.12116948392642826, 0.12130798445717574, 0.12126608918616512, 0.12143957592976222, 0.12144568786916807, 0.12149277244377231, 0.12139803373011432, 0.12154122379838782, 0.1215672847487258, 0.12185458602082576, 0.12176740412710842, 0.12192876764785865, 0.12180257172914707, 0.12172485722743659, 0.12199966702115489, 0.12205731205574921, 0.12204635822924047, 0.12223485936330918, 0.12206782790122701, 0.12233998491156854, 0.1222912921812349, 0.12239703900180415, 0.12242123775912887, 0.12249417392571559, 0.12240201258576007, 0.12246355868807086, 0.12257586143733647, 0.12256044263096555, 0.12256767815383114, 0.12264464858027053, 0.12263451480383651, 0.12270181882075121, 0.12268330129878223, 0.1228011812477962, 0.12272693870017633, 0.12282380935002948, 0.12292887229117695, 0.1228483270609263, 0.12294015177797973, 0.12296578561918978, 0.12297495656181695, 0.12307263271577881, 0.12307508377937211, 0.12308756135984368, 0.12314350440971907, 0.12307041167874104, 0.12319824041381253, 0.12323341337309103, 0.12322600654339794, 0.12325315244890817, 0.12330759019340348, 0.12327638063909035, 0.12326110681979116]
val_acc = [0.9456, 0.9618, 0.9695, 0.9732, 0.9762, 0.9788, 0.979, 0.9786, 0.9789, 0.9797, 0.9813, 0.9785, 0.9813, 0.9801, 0.9798, 0.9831, 0.9816, 0.9824, 0.9817, 0.9821, 0.982, 0.982, 0.9792, 0.9803, 0.9817, 0.9824, 0.9819, 0.9825, 0.9806, 0.9822, 0.9821, 0.9819, 0.9823, 0.9823, 0.9813, 0.9819, 0.9811, 0.9812, 0.9819, 0.982, 0.9819, 0.9818, 0.982, 0.9824, 0.9818, 0.9825, 0.9823, 0.979, 0.9806, 0.9816, 0.9816, 0.9818, 0.9818, 0.982, 0.9819, 0.9821, 0.9822, 0.9821, 0.9819, 0.9824, 0.9822, 0.982, 0.9814, 0.9812, 0.9817, 0.982, 0.982, 0.9819, 0.9821, 0.9821, 0.9822, 0.982, 0.9826, 0.9822, 0.9817, 0.9824, 0.9822, 0.9818, 0.9822, 0.9822, 0.9826, 0.982, 0.9825, 0.9821, 0.9821, 0.9811, 0.9817, 0.9817, 0.9816, 0.9814, 0.9815, 0.9818, 0.9815, 0.9818, 0.9819, 0.9819, 0.9817, 0.9819, 0.982, 0.9821, 0.9825, 0.9823, 0.9818, 0.9823, 0.9819, 0.9821, 0.9817, 0.982, 0.9822, 0.9819, 0.9821, 0.9819, 0.9822, 0.9817, 0.9823, 0.9822, 0.982, 0.982, 0.9823, 0.9821, 0.9819, 0.9823, 0.9821, 0.9822, 0.9822, 0.9819, 0.9821, 0.9821, 0.982, 0.9821, 0.9823, 0.9822, 0.982, 0.982, 0.9819, 0.9817, 0.9819, 0.982, 0.9821, 0.9821, 0.982, 0.9821, 0.9819, 0.9819, 0.9822, 0.982, 0.982, 0.9821, 0.9819, 0.9822, 0.9822, 0.9821, 0.982, 0.9821, 0.9821, 0.9821, 0.9822, 0.9819, 0.982, 0.9821, 0.982, 0.9822, 0.9822, 0.9821, 0.982, 0.9823, 0.9822, 0.9822, 0.9822, 0.9822, 0.9823, 0.982, 0.982, 0.9822, 0.9821, 0.9822, 0.9822, 0.9822, 0.9823, 0.9821, 0.9823, 0.9821, 0.9822, 0.9822, 0.9821, 0.9823, 0.9822, 0.9823, 0.9822, 0.9822, 0.9822, 0.9822, 0.9823, 0.9822, 0.9823, 0.9822, 0.9822, 0.9822, 0.9822, 0.9822]
lr = [1e-04, 1e-04, 9.7e-05, 9.7e-05, 9.409e-05, 9.409e-05, 9.12673e-05, 9.12673e-05, 8.852928e-05, 8.852928e-05, 8.5873406e-05, 8.5873406e-05, 8.32972e-05, 8.32972e-05, 8.079829e-05, 8.079829e-05, 7.837434e-05, 7.837434e-05, 7.6023105e-05, 7.6023105e-05, 7.374241e-05, 7.374241e-05, 7.153014e-05, 7.153014e-05, 6.938424e-05, 6.938424e-05, 6.730271e-05, 6.730271e-05, 6.5283624e-05, 6.5283624e-05, 6.332512e-05, 6.332512e-05, 6.1425366e-05, 6.1425366e-05, 5.9582606e-05, 5.9582606e-05, 5.7795125e-05, 5.7795125e-05, 5.606127e-05, 5.606127e-05, 5.4379434e-05, 5.4379434e-05, 5.274805e-05, 5.274805e-05, 5.116561e-05, 5.116561e-05, 4.9630642e-05, 4.9630642e-05, 4.8141723e-05, 4.8141723e-05, 4.669747e-05, 4.669747e-05, 4.5296547e-05, 4.5296547e-05, 4.393765e-05, 4.393765e-05, 4.2619522e-05, 4.2619522e-05, 4.1340936e-05, 4.1340936e-05, 4.0100706e-05, 4.0100706e-05, 3.8897684e-05, 3.8897684e-05, 3.7730755e-05, 3.7730755e-05, 3.659883e-05, 3.659883e-05, 3.5500867e-05, 3.5500867e-05, 3.443584e-05, 3.443584e-05, 3.3402765e-05, 3.3402765e-05, 3.2400683e-05, 3.2400683e-05, 3.1428663e-05, 3.1428663e-05, 3.0485802e-05, 3.0485802e-05, 2.9571229e-05, 2.9571229e-05, 2.8684091e-05, 2.8684091e-05, 2.782357e-05, 2.782357e-05, 2.6988862e-05, 2.6988862e-05, 2.6179196e-05, 2.6179196e-05, 2.539382e-05, 2.539382e-05, 2.4632005e-05, 2.4632005e-05, 2.3893046e-05, 2.3893046e-05, 2.3176253e-05, 2.3176253e-05, 2.2480966e-05, 2.2480966e-05, 2.1806538e-05, 2.1806538e-05, 2.1152342e-05, 2.1152342e-05, 2.0517771e-05, 2.0517771e-05, 1.9902238e-05, 1.9902238e-05, 1.9305171e-05, 1.9305171e-05, 1.8726016e-05, 1.8726016e-05, 1.8164235e-05, 1.8164235e-05, 1.7619308e-05, 1.7619308e-05, 1.709073e-05, 1.709073e-05, 1.6578007e-05, 1.6578007e-05, 1.6080667e-05, 1.6080667e-05, 1.5598247e-05, 1.5598247e-05, 1.5130299e-05, 1.5130299e-05, 1.46763905e-05, 1.46763905e-05, 1.4236099e-05, 1.4236099e-05, 1.3809015e-05, 1.3809015e-05, 1.3394745e-05, 1.3394745e-05, 1.2992903e-05, 1.2992903e-05, 1.26031155e-05, 1.26031155e-05, 1.2225022e-05, 1.2225022e-05, 1.1858272e-05, 1.1858272e-05, 1.1502523e-05, 1.1502523e-05, 1.1157448e-05, 1.1157448e-05, 1.0822724e-05, 1.0822724e-05, 1.0498043e-05, 1.0498043e-05, 1.0183101e-05, 1.0183101e-05, 9.877608e-06, 9.877608e-06, 9.58128e-06, 9.58128e-06, 9.293842e-06, 9.293842e-06, 9.015026e-06, 9.015026e-06, 8.744576e-06, 8.744576e-06, 8.482239e-06, 8.482239e-06, 8.227771e-06, 8.227771e-06, 7.980938e-06, 7.980938e-06, 7.74151e-06, 7.74151e-06, 7.509265e-06, 7.509265e-06, 7.2839866e-06, 7.2839866e-06, 7.0654673e-06, 7.0654673e-06, 6.8535032e-06, 6.8535032e-06, 6.647898e-06, 6.647898e-06, 6.448461e-06, 6.448461e-06, 6.255007e-06, 6.255007e-06, 6.067357e-06, 6.067357e-06, 5.885336e-06, 5.885336e-06, 5.7087764e-06, 5.7087764e-06, 5.537513e-06, 5.537513e-06, 5.3713875e-06, 5.3713875e-06, 5.210246e-06, 5.210246e-06, 5.0539384e-06, 5.0539384e-06, 4.9023206e-06, 4.9023206e-06]

loss

#plot_4_history(loss,acc,val_loss,val_acc)
#plot_one_thing(lr[:10],"Epoch","Learning Rate","Step Decay Learning Rate")



# plot(adam_train_acc)
# plot(adam_097decayLoss)

# plot(adam_097decayLoss,'loss',adam_097decayAccuracy,'accuracy')


#plot(tdecay001,'0.001 decay every iteration',nodecay_50,'no decay')

#
# plt.plot(dim,[0.9168, 0.9274, 0.9339, 0.9399, 0.9439, 0.9465, 0.9485, 0.9499, 0.9509, 0.9469], 'bo--')
# plt.plot(dim,[0.9184, 0.9302, 0.9378, 0.9443, 0.9468, 0.9466, 0.9491, 0.9504, 0.9532, 0.9535], 'bo--',label='Scaling')
# plt.xticks(dim)
# plt.title('Effect of Scaling')
# plt.legend(loc='lower right')
# plt.xlabel("Epoch")
# plt.ylabel("Accuracy")
# plt.show()
#
# t = np.arange(0., 11)


