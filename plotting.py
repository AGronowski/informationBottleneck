import tensorflow as tf
from tensorflow import keras
import numpy as np
import matplotlib.pyplot as plt

# Plot incorrect images
# Setting incorrecOnly to True plots only wrong predictions
def plot_images_with_prediction(predictions_array, labels, images, row, column, incorrect_only=False):
    # Plot grid of images
    num_row = row
    num_column = column
    num_plotted_images = num_row * num_column
    plt.figure(figsize=(2 * 2 * num_column, 2 * num_row))

    num_total_images = len(labels)

    # Plot incorrect predictions with error bar
    j = 0
    for i in range(num_plotted_images):
        if incorrect_only:
            while j < num_total_images - 1 and np.argmax(predictions_array[j]) == labels[j]:
                j += 1
        if j != num_total_images - 1:
            plt.subplot(num_row, 2 * num_column, 2 * i + 1)
            plot_image(j, predictions_array, labels, images)
            plt.subplot(num_row, 2 * num_column, 2 * i + 2)
            plot_value_array(j, predictions_array, labels, "Predicted Label")
            j += 1
    plt.show()


# Calculate number of wrong images
def get_num_wrong(predictions_array, label_array):
    numWrong = 0
    numImages = label_array.shape[0]
    for i in range(numImages):
        if np.argmax(predictions_array[i]) != label_array[i]:
            numWrong += 1

    print('numWrong ' + str(numWrong))
    print('wrong ' + str(numWrong / numImages * 100) + '%')


# Calculate number of predictions that have 100 percent confidence
def get_num_100_predictions(predictions_array):
    length = predictions_array.shape[0]
    num100Confident = 0
    for i in range(length):
        if predictions_array[i][np.argmax(predictions_array[i])] == 1:
            num100Confident += 1
    return num100Confident


# Plot image
def plot_image(i, predictions_array, true_label, img, class_names):
    predictions_array, true_label, img = predictions_array[i], true_label[i], img[i]
    plt.grid(False)
    plt.xticks([])
    plt.yticks([])

    plt.imshow(img, cmap=plt.cm.binary)

    predicted_label = np.argmax(predictions_array)
    if predicted_label == true_label:
        color = 'blue'
    else:
        color = 'red'

    plt.xlabel("Predicted {} ({}) \n {:2.1f}% confidence".format(class_names[predicted_label], class_names[true_label],
                                                                 100 * np.max(predictions_array)),
               color=color)


# Plot prediction distribution
def plot_value_array(i, predictions_array, true_label, title=''):
    predictions_array, true_label = predictions_array[i], true_label[i]
    plt.grid(False)
    plt.xticks(np.arange(0, 10, step=1))
    # plt.yticks(np.arange(0, 2, step=1))
    thisplot = plt.bar(range(10), predictions_array, color="#777777")
    plt.ylim([0, 1])
    predicted_label = np.argmax(predictions_array)
    thisplot[predicted_label].set_color('red')
    thisplot[true_label].set_color('blue')
    plt.title(title)


# Plot prediction distribution with ground truth distribution
def plot_with_ground_truth(predictions, labels, images, row):
    one_hot_encodings = keras.utils.to_categorical(labels)

    for i in range(row):
        plt.subplot(row, 3, 3 * i + 1)
        plot_image(i, predictions, labels, images)

        plt.subplot(row, 3, 3 * i + 2)
        plot_value_array(i, predictions, labels, "Predicted Label")

        plt.subplot(row, 3, 3 * i + 3)
        plot_value_array(i, one_hot_encodings, labels, "Ground Truth")

    plt.show()


# Plot anything
def plot(input1, l1='', input2=None, l2='', input3=None, l3='', input4=None, l4='', input5=None, l5='',input6=None, l6='',input7=None, l7=''):

    epochs = len(input1)
    dim = np.arange(1, epochs + 1)
    plt.plot(dim, input1, 'o--', label= l1)
    if input2:
        plt.plot(dim, input2, 'o--', label=l2)
    if input3:
        plt.plot(dim, input3, 'o--', label=l3)
    if input4:
        plt.plot(dim, input4, 'o--', label=l4)
    if input5:
        plt.plot(dim, input5, 'o--', label=l5)
    if input6:
        plt.plot(dim, input6, 'o--', label=l6)
    if input7:
        plt.plot(dim, input7, 'o--', label=l7)

    #plt.xticks(dim)
    plt.title('Adam Optimizer')
    plt.legend(loc='best')
    plt.xlabel("Epoch")
    plt.ylabel("Accuracy")
    plt.show()


def plot_4_history(train_loss, train_acc, val_loss, val_acc, start=0, end=None):
    if not end:
        end = len(train_loss)
    train_loss = train_loss[start:end]
    train_acc = train_acc[start:end]
    val_loss = val_loss[start:end]
    val_acc = val_acc[start:end]

    dim = np.arange(start + 1, end + 1)

    marker = 'o--'
    if end - start > 50:
        marker = '--'


    plt.subplot(1, 2, 1)
    plt.title("Cross-Entropy Loss")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.plot(dim, train_loss, marker, label="Training Set")
    # plt.xticks(dim)

    # val loss
    plt.plot(dim, val_loss, marker, label="Test Set")
    plt.legend(loc='best')

    plt.subplot(1, 2, 2)
    plt.title("Accuracy")
    plt.xlabel("Epoch")
    plt.ylabel("Accuracy")
    # plt.xticks(dim)

    # acc
    plt.plot(dim, train_acc, marker, label="Training Set")
    plt.plot(dim, val_acc, marker, label="Test Set")
    plt.legend(loc='best')
    plt.show()

# Plot one thing
def plot_one_thing(data,xlabel="",ylabel="",title="",legend_label= ""):

    epoch = len(data)
    dim = np.arange(1, epoch + 1)
    plt.plot(dim, val_loss, 'o--', label=legend_label)
    plt.legend(loc='best')
    plt.xlabel(xlabel)
    plt.ylabel(ylabel)
    plt.title(title)
    plt.show()


adam_097decayLoss = [
1.717717946320772293e-01, 1.171987988799810393e-01, 9.423242220655084211e-02, 8.223272252827883155e-02, 7.790312736295162965e-02, 6.795573371462523826e-02, 6.661978335119783656e-02, 6.416869927663355899e-02, 6.421787718776612863e-02, 5.750558819156140200e-02, 6.313139389269054047e-02, 6.029642598703503775e-02, 6.125896084588020735e-02, 6.608219466754235161e-02, 6.543550509493797906e-02, 6.625056594901251872e-02, 6.712385269929654896e-02, 6.724560067523270568e-02, 7.264187593769748374e-02, 6.951213151216506514e-02, 7.003076932232943363e-02, 7.118484503291255816e-02, 9.265953672584145706e-02, 7.666345472534885630e-02, 7.332408048736396844e-02, 7.408003638687660208e-02, 7.520465349647566244e-02, 7.705951488256687631e-02, 1.071122423638007637e-01, 8.514242686826037354e-02, 7.757937876653159259e-02, 7.863521644292631452e-02, 7.954927285012672322e-02, 8.015847423106606251e-02, 8.182655942619021516e-02, 8.300534108740975636e-02, 8.343910811112437154e-02, 8.581603221944533733e-02, 8.382435017352690698e-02, 8.305637310637394022e-02, 8.348388741913367883e-02, 8.369942272081970502e-02, 8.516454428008728150e-02, 8.551265524452901501e-02, 8.597343966759125877e-02, 8.677568994127796143e-02, 8.740678448049038485e-02, 8.876028600643622035e-02, 8.936793023887730503e-02, 1.027117557355086280e-01, 9.126226551445143498e-02, 9.099367414986772595e-02, 9.095809954509605222e-02, 9.115127237266279492e-02, 9.140655386379875624e-02, 9.179129923446453787e-02, 9.179047623896621766e-02, 9.217723769286749469e-02, 9.259211558577590262e-02, 9.351508112733426803e-02, 9.382205254267318717e-02, 9.391459235805087180e-02, 9.473033510361346388e-02, 9.558503825504885421e-02, 9.656431099117526728e-02, 9.843540056863275822e-02, 9.751456019389934116e-02, 9.897726848020284296e-02, 9.985505653748986965e-02, 1.229557371713963232e-01, 1.072941994842585706e-01, 1.046531711712304913e-01, 1.035886796799968534e-01, 1.034915134572656770e-01, 1.031932875393613419e-01, 1.031985297289156750e-01, 1.033098525820150826e-01, 1.032397052940041277e-01, 1.031957925862434372e-01, 1.033223852539595089e-01, 1.035098667870442968e-01, 1.033715042950370810e-01, 1.040533006683835998e-01, 1.042528235562122096e-01, 1.045921649388856234e-01, 1.047061356798758419e-01, 1.052166655204973683e-01, 1.059910783017130598e-01, 1.059926238100433737e-01, 1.063794797078662646e-01, 1.076631636345155157e-01, 1.081626373208064290e-01, 1.085566045746600150e-01, 1.096565727289760123e-01, 1.101043984092848343e-01, 1.105741878683284135e-01, 1.108438639984160590e-01, 1.110877840977264053e-01, 1.121585265446628926e-01, 1.130659220052257358e-01, 1.124454514840181191e-01, 1.139007207674956196e-01, 1.140827342957527468e-01, 1.144716231716347427e-01, 1.144386725118226866e-01, 1.151296252717656926e-01, 1.148563483879797842e-01, 1.156297001948723840e-01, 1.158956403641923372e-01, 1.166025851799395302e-01, 1.168859047925616262e-01, 1.165978109434399751e-01, 1.175411607582600437e-01, 1.175294661989642053e-01, 1.175614235249054229e-01, 1.175340454874253676e-01, 1.181557613260883904e-01, 1.182407720530954942e-01, 1.189413881344421470e-01, 1.186244803670911119e-01, 1.189723288925415567e-01, 1.188277518488666806e-01, 1.190858438509530964e-01, 1.191612457098199240e-01, 1.195745069269014554e-01, 1.196243999974171829e-01, 1.196249641969916455e-01, 1.198896729084390211e-01, 1.199316698026359485e-01, 1.199880160304746812e-01, 1.201636573397877328e-01, 1.203678988336395195e-01, 1.203302374398663166e-01, 1.203595578521344522e-01, 1.204924834591724891e-01, 1.208469351531312047e-01, 1.207642376157207348e-01, 1.207821243693752017e-01, 1.208602299601761881e-01, 1.210054668352313484e-01, 1.210665770508217026e-01, 1.211829611040002447e-01, 1.212837827987095773e-01, 1.213270489434375987e-01, 1.211575815888841084e-01, 1.213935756328544391e-01, 1.215040125902885348e-01, 1.217188749106162537e-01, 1.216716543812322027e-01, 1.215747942516963997e-01, 1.218506344645186312e-01, 1.216932494073012100e-01, 1.217942648318171422e-01, 1.217919149717249655e-01, 1.218665962723343849e-01, 1.219416738491872765e-01, 1.219833172021935752e-01, 1.221077115500565913e-01, 1.221059251731548972e-01, 1.220926709115383313e-01, 1.221680885320927251e-01, 1.222133082791788117e-01, 1.223394563538033747e-01, 1.223737193597480705e-01, 1.224032569680161969e-01, 1.223652106215649821e-01, 1.224604167723259696e-01, 1.224165206605982853e-01, 1.224267062226044867e-01, 1.224956120967681250e-01, 1.225734149152208352e-01, 1.226346133887341178e-01, 1.227353116063841232e-01, 1.226959532804221403e-01, 1.226744427599652520e-01, 1.227666029581246238e-01, 1.228046398517562976e-01, 1.228938798577121350e-01, 1.228692238208509424e-01, 1.228300462922015807e-01, 1.229100947280369338e-01, 1.228920704723634599e-01, 1.229278680877178009e-01, 1.230088297926977270e-01, 1.230378422748401590e-01, 1.229904383699977205e-01, 1.230379357770948479e-01, 1.230338161155805216e-01, 1.230747677669387502e-01, 1.231262655549824059e-01, 1.231258880725931704e-01, 1.231494153608218706e-01, 1.231622304898200987e-01, 1.231654653985515269e-01, 1.232267704874425301e-01, 1.232112642745323700e-01, 1.232581074862719173e-01, 1.232953481603260570e-01, 1.233574941089102539e-01, 1.233234502678237782e-01
]


adam_097decayAccuracy = [0.9522, 0.9643, 0.9713, 0.9752, 0.9763, 0.9778, 0.9785, 0.9798, 0.9804, 0.9826, 0.9815, 0.9822, 0.9816, 0.9805, 0.9818, 0.9823, 0.9819, 0.9833, 0.9813, 0.9823, 0.9825, 0.9831, 0.9788, 0.9829, 0.9829, 0.9833, 0.9833, 0.983, 0.9754, 0.9806, 0.9832, 0.9831, 0.9838, 0.9833, 0.9835, 0.9834, 0.9831, 0.9816, 0.9825, 0.9829, 0.9831, 0.9833, 0.9828, 0.9832, 0.9832, 0.9834, 0.9833, 0.9832, 0.9836, 0.9801, 0.983, 0.9821, 0.9826, 0.9827, 0.9829, 0.9829, 0.9831, 0.983, 0.983, 0.9828, 0.983, 0.983, 0.9828, 0.983, 0.9827, 0.9827, 0.9831, 0.9833, 0.9831, 0.9795, 0.9814, 0.9832, 0.9831, 0.9831, 0.983, 0.9832, 0.9831, 0.9834, 0.9837, 0.9837, 0.9832, 0.9832, 0.983, 0.9834, 0.9832, 0.9836, 0.9835, 0.9834, 0.9835, 0.9837, 0.9834, 0.9831, 0.9833, 0.9833, 0.9834, 0.9836, 0.9832, 0.9836, 0.9834, 0.9835, 0.9837, 0.9833, 0.9832, 0.9831, 0.9836, 0.9834, 0.983, 0.9833, 0.9831, 0.983, 0.9829, 0.9832, 0.983, 0.983, 0.983, 0.983, 0.9832, 0.9827, 0.9829, 0.9829, 0.9829, 0.9832, 0.983, 0.983, 0.9828, 0.9828, 0.9828, 0.9828, 0.9829, 0.9827, 0.9829, 0.9829, 0.9828, 0.9831, 0.9829, 0.9825, 0.9829, 0.9831, 0.9827, 0.9828, 0.9826, 0.9826, 0.9825, 0.9826, 0.9827, 0.9826, 0.9828, 0.9826, 0.9826, 0.9825, 0.9826, 0.9827, 0.9826, 0.9826, 0.9826, 0.9828, 0.9827, 0.9826, 0.9826, 0.9827, 0.9826, 0.9827, 0.9826, 0.9826, 0.9826, 0.9826, 0.9826, 0.9826, 0.9827, 0.9827, 0.9826, 0.9827, 0.9826, 0.9826, 0.9826, 0.9827, 0.9827, 0.9826, 0.9827, 0.9826, 0.9827, 0.9827, 0.9826, 0.9826, 0.9826, 0.9827, 0.9827, 0.9827, 0.9827, 0.9827, 0.9826, 0.9828, 0.9827, 0.9827, 0.9827, 0.9827, 0.9826, 0.9826, 0.9827, 0.9827]

adam_Loss = [
1.796218045800924423e-01, 1.287161749228835084e-01, 9.400671184062957264e-02, 8.157250314950942327e-02, 7.744802344590424881e-02, 7.109144912101328373e-02, 6.606426751241087358e-02, 6.639816710725426896e-02, 6.304746644338593553e-02, 6.439137598266825302e-02, 6.317465959349646831e-02, 6.426404737634584807e-02, 6.995447941590100804e-02, 6.744170086691156363e-02, 7.330808359081857073e-02, 7.018717127968557290e-02, 7.710094711219425689e-02, 7.099172161717433316e-02, 7.117038972384762130e-02, 8.174543735047336279e-02, 6.743843840580666937e-02, 6.887856298999395221e-02, 7.839737816306296958e-02, 7.069627823664631494e-02, 7.068690438049088365e-02, 7.236837325122906506e-02, 7.352938877942506724e-02, 7.257163550297264509e-02, 7.166399030858883190e-02, 7.256651801944827285e-02, 7.292282848226022685e-02, 7.489533391486474789e-02, 7.737275774314184007e-02, 7.779201486075180971e-02, 1.025964662618935042e-01, 7.830090649280464776e-02, 7.602159266825765649e-02, 7.647015520982676506e-02, 7.765126569851417415e-02, 7.846588650398189646e-02, 7.990600132419785340e-02, 8.067395011647021663e-02, 8.177699566760565641e-02, 8.322208772705380619e-02, 1.106527500296942829e-01, 8.112259650308260850e-02, 8.180612217667658503e-02, 8.105793600203468896e-02, 8.152047984032834149e-02, 8.300670192773396128e-02, 8.411695243179564963e-02, 8.504511549028392903e-02, 8.626810866146115975e-02, 8.789551822141219994e-02, 8.871996243594822140e-02, 9.045163884780776931e-02, 9.086989902170898581e-02, 9.388841753007000357e-02, 9.375259653261310000e-02, 9.637247507772826738e-02, 9.573792455696093773e-02, 9.691363138330416627e-02, 1.043682891869684715e-01, 9.548521334956604212e-02, 9.363927342128590403e-02, 9.412777044213725686e-02, 9.468091514622756222e-02, 9.527584719763466936e-02, 9.598016791343980114e-02, 9.662194517237367164e-02, 9.782732549269894384e-02, 9.865711632590318247e-02, 9.945612652636555417e-02, 1.003527896234299999e-01, 1.015698735761907251e-01, 1.023109567566302808e-01, 1.044372215255862102e-01, 1.053563121304159772e-01, 1.061620478843872256e-01, 1.064256481581787733e-01, 1.073876301656309790e-01, 1.087532477389549918e-01, 2.346934477705508448e-01, 1.007099648977964473e-01, 9.815796729099020290e-02, 9.831228021861533606e-02, 9.848278234645477247e-02, 9.887747872267936555e-02, 9.925561818377900969e-02, 9.972039816796496314e-02, 1.004617594746377990e-01, 1.009205880907757119e-01, 1.019984233677985186e-01, 1.025337685649174158e-01, 1.034481925786985834e-01, 1.042986297541870083e-01, 1.048102331683571781e-01, 1.058975795801961745e-01, 1.067115432718492296e-01, 1.079008745639953304e-01, 1.083686319700973427e-01, 1.098460012736177244e-01, 3.705197997030336143e-01, 1.279334978267070155e-01, 1.095597647509657657e-01, 1.087545119793958265e-01, 1.071152282173170600e-01, 1.071113925640543169e-01, 1.077062330508488236e-01, 1.076424101155953328e-01, 1.079526770919437689e-01, 1.084143321648389224e-01, 1.085676793667335049e-01, 1.092335098433704910e-01, 1.096736824339132055e-01, 1.102902929820947925e-01, 1.106993470169436444e-01, 1.114489329183504657e-01, 1.124464819802187820e-01, 1.132176020182896481e-01, 1.130679095855805366e-01, 1.137297833120555907e-01, 1.148989107928900938e-01, 1.152389321182454296e-01, 1.165802009798699163e-01, 1.167073710780550805e-01, 1.170855740127490219e-01, 1.185126124639850759e-01, 1.188018192841445858e-01, 1.191425204591587267e-01, 1.200298048006970958e-01, 1.226785895192187975e-01, 1.278199395599858568e-01, 1.183082361578362968e-01, 1.246829421454503928e-01, 1.672451627824193543e-01, 1.197951034361449257e-01, 1.165214574317451485e-01, 1.107209162218237009e-01, 1.097890138413981781e-01, 1.097695010473307836e-01, 1.096785731642648948e-01, 1.098271858292537151e-01, 1.096093811551656916e-01, 1.097842327398757395e-01, 1.101596988349501788e-01, 1.101596537409613996e-01, 1.104157154396216423e-01, 1.109039101511086756e-01, 1.114246604044797084e-01, 1.119535760330995777e-01, 1.124232718658981917e-01, 1.129948739160340637e-01, 1.136099692759819368e-01, 1.146333698242369831e-01, 1.152684249491335300e-01, 1.161536347546029968e-01, 1.168983248144468273e-01, 1.175821089391752144e-01, 1.182675946106951370e-01, 1.188719983271062097e-01, 1.201568807984722265e-01, 1.213218721873306483e-01, 1.215394681280258793e-01, 1.226569673754465822e-01, 1.233156514652080232e-01, 1.239744430245366402e-01, 1.247335747378303938e-01, 1.248942802006854369e-01, 1.252826591217626739e-01, 1.261176098489272535e-01, 1.267487741897969089e-01, 1.265724311394796908e-01, 1.273073453298741164e-01, 1.275793887563440865e-01, 1.275287493358899793e-01, 1.279290155032674625e-01, 1.285073899865568636e-01, 1.285691839118953683e-01, 1.286263742602625493e-01, 1.291044721913385818e-01, 1.290222323086288592e-01, 1.292665524784479270e-01, 1.295455999136038427e-01, 1.296025093589196209e-01, 1.296298045144828881e-01, 1.299966706877125899e-01, 1.301570612750174438e-01, 1.301936462070659717e-01, 1.303907597951092512e-01, 1.305585812606989415e-01, 1.304167019568785724e-01, 1.308890836629996168e-01, 1.309334799835312479e-01, 1.308422897125342266e-01, 1.310069455836696817e-01, 1.311126247862182403e-01, 1.309987319218362256e-01, 1.312968800982196038e-01, 1.313862343124458099e-01
]

adam_Accuracy = [0.949, 0.9617, 0.9724, 0.9757, 0.9744, 0.979, 0.9795, 0.9772, 0.9798, 0.9805, 0.9805, 0.9807, 0.9787, 0.9808, 0.9792, 0.9807, 0.9798, 0.9805, 0.9811, 0.9794, 0.9833, 0.983, 0.9797, 0.9835, 0.9831, 0.983, 0.9824, 0.9834, 0.9829, 0.9822, 0.9829, 0.9828, 0.9827, 0.9828, 0.977, 0.9827, 0.9835, 0.9839, 0.9837, 0.9837, 0.9836, 0.9833, 0.9835, 0.9832, 0.9779, 0.9816, 0.9825, 0.9826, 0.9826, 0.9827, 0.9826, 0.9824, 0.9829, 0.9823, 0.9825, 0.9826, 0.9833, 0.9824, 0.9828, 0.9829, 0.9828, 0.9826, 0.9802, 0.982, 0.9818, 0.982, 0.9821, 0.9824, 0.9826, 0.9825, 0.9826, 0.9829, 0.9826, 0.9824, 0.9824, 0.9825, 0.9823, 0.9828, 0.9822, 0.9825, 0.9825, 0.9829, 0.9662, 0.9824, 0.9822, 0.982, 0.9822, 0.9825, 0.9829, 0.9828, 0.9826, 0.983, 0.9829, 0.9829, 0.9836, 0.9832, 0.9833, 0.9831, 0.9831, 0.9832, 0.9833, 0.9827, 0.9521, 0.9795, 0.982, 0.9828, 0.9835, 0.9835, 0.9836, 0.9834, 0.9836, 0.9835, 0.9835, 0.9836, 0.9832, 0.9833, 0.983, 0.9831, 0.983, 0.9832, 0.9832, 0.9834, 0.9835, 0.9835, 0.9834, 0.9834, 0.9833, 0.9834, 0.9836, 0.9834, 0.9834, 0.9798, 0.981, 0.9813, 0.9816, 0.9755, 0.9819, 0.9813, 0.9822, 0.9823, 0.9823, 0.9825, 0.9827, 0.9827, 0.9825, 0.9828, 0.9825, 0.9827, 0.9827, 0.9828, 0.9827, 0.9827, 0.9827, 0.9826, 0.9827, 0.9829, 0.9828, 0.9824, 0.9824, 0.9824, 0.9824, 0.9821, 0.9825, 0.9826, 0.9823, 0.9824, 0.9823, 0.9827, 0.9823, 0.9826, 0.9827, 0.9824, 0.9824, 0.9825, 0.9826, 0.9823, 0.9825, 0.9824, 0.9822, 0.9824, 0.9825, 0.9824, 0.9824, 0.9826, 0.9823, 0.9824, 0.9824, 0.9825, 0.9824, 0.9824, 0.9824, 0.9824, 0.9824, 0.9824, 0.9823, 0.9824, 0.9826, 0.9825, 0.9826, 0.9826]


adam_train_loss = [0.3672918666402499, 0.14632374417036773, 0.10083533977468809, 0.07507594500202686, 0.057295409271803995, 0.045502985861773294, 0.03519346915728723, 0.027327967220141244, 0.02153883931110613, 0.016797154115629383, 0.013708370206295513, 0.010119287097477354, 0.007563361768407048, 0.006343274504082122, 0.0054814483854473415, 0.0037939516374414476, 0.0025722732161617993, 0.00536578716681106, 0.0020328473763341513, 0.0019474277205881662, 0.005366528725280659, 0.001031738727475992, 0.000820693962432415, 0.0019669887012059915, 0.0003691491296831373, 0.0002580144957876958, 0.0002239053917946876, 0.008378714645368745, 0.0005447156246570254, 0.00025875982239692043, 0.00017986047045572681, 0.00014274218461954053, 0.00011713895058922692, 9.442349391974858e-05, 0.004115142977017664, 0.0047432215870564206, 0.0003816010474285273, 0.0001801688511386601, 0.00012550177528434383, 9.764323703317737e-05, 7.792673068630999e-05, 6.328475469596622e-05, 5.144803068181621e-05, 4.3706812352108194e-05, 0.006562833613911228, 0.003025563441953333, 0.0003169195679477828, 0.00012583555922446977, 8.585956127717509e-05, 6.814660408736018e-05, 5.506520030924851e-05, 4.460406079980809e-05, 3.6324093727747216e-05, 2.9678697588527334e-05, 2.454448169828538e-05, 2.0426104796721726e-05, 1.6519900646774962e-05, 1.4221106636872113e-05, 1.1557323256852214e-05, 9.686962961457842e-06, 7.724462680206064e-06, 6.647308063634228e-06, 0.009445087306141886, 0.0006491227568617812, 8.747856104908653e-05, 4.796751298329127e-05, 3.780369445166798e-05, 3.0526137458840214e-05, 2.508325469553559e-05, 2.05259335861759e-05, 1.6885016770136947e-05, 1.3888356359643695e-05, 1.1500523510221682e-05, 9.490559506086053e-06, 7.89718550095131e-06, 6.585770369724742e-06, 5.512763549404554e-06, 4.702571557497019e-06, 4.021302992782694e-06, 3.4596707289817156e-06, 2.9237973562127688e-06, 2.517554690560549e-06, 0.0007727591179038502, 0.009818299263386052, 0.00010137437990958157, 3.8396833072814234e-05, 2.839199069001097e-05, 2.2885360959700543e-05, 1.8829797314955008e-05, 1.5594478058271002e-05, 1.294038492630989e-05, 1.0788631447553598e-05, 8.978300980212832e-06, 7.496879268463393e-06, 6.234969168872339e-06, 5.243866124449899e-06, 4.395862296936078e-06, 3.7430231168400496e-06, 3.1868446793244705e-06, 2.7350962123288505e-06, 2.3827460218702375e-06, 2.1110344321565814e-06, 1.681519054348731e-05, 0.009779710111252483, 0.00039672597573712663, 8.313815376482126e-05, 2.7664134509564065e-05, 1.553595328251352e-05, 1.280325884560322e-05, 1.0650113859848412e-05, 9.091530745498252e-06, 7.732378966428162e-06, 6.569834909745017e-06, 5.561816089236042e-06, 4.730414568712149e-06, 4.017288751470005e-06, 3.4326101825854495e-06, 2.9424682702483552e-06, 2.560306243329554e-06, 2.2419239110149646e-06, 1.9938424553818855e-06, 1.7976243752097312e-06, 1.633404940359166e-06, 1.5054573827910644e-06, 1.402727643835533e-06, 1.3191877489286224e-06, 1.2529203812050582e-06, 1.20047315363081e-06, 1.1561098898482669e-06, 1.1206077468273179e-06, 1.0942725275716232e-06, 0.010614025529619748, 0.001446519416761968, 0.00016080173523543332, 6.917429391194219e-05, 0.0014461631507752296, 0.0026053141317676893, 0.0007846112362453066, 0.00021768471750609327, 1.502379443384901e-05, 1.0522528651411752e-05, 8.626572926573317e-06, 7.171092349267383e-06, 6.138381178478388e-06, 5.209804653721524e-06, 4.49737609320285e-06, 3.874472836287168e-06, 3.35580919208193e-06, 2.91827653920033e-06, 2.5604573392001838e-06, 2.2654480968261243e-06, 2.0107620555146847e-06, 1.8016521520773192e-06, 1.637321772515558e-06, 1.5027950720991612e-06, 1.387550963348379e-06, 1.3058466050589838e-06, 1.2348188287584586e-06, 1.1782251122364565e-06, 1.1358347118554472e-06, 1.0989219279622375e-06, 1.072088120395165e-06, 1.0497484426726562e-06, 1.0328088748678965e-06, 1.0178163995533396e-06, 1.0066087807558688e-06, 9.980932945078772e-07, 9.900149053502598e-07, 9.845908975118315e-07, 9.794370979913463e-07, 9.75670098550078e-07, 9.723918509507713e-07, 9.696381237252657e-07, 9.674804406737773e-07, 9.65883036390854e-07, 9.643114638417197e-07, 9.63083608667148e-07, 9.620266195042859e-07, 9.610789072667104e-07, 9.605365022480329e-07, 9.597079980494526e-07, 9.59215266789215e-07, 9.587086241443405e-07, 9.582814623361932e-07, 9.578840954797366e-07, 9.575244832641753e-07, 9.571986421027153e-07, 9.569403560287053e-07, 9.566761089748373e-07, 9.564655063816947e-07, 9.562509291072274e-07, 9.560880091423012e-07, 9.559270784128178e-07, 9.557939610734441e-07, 9.556588574355373e-07, 9.55462161205863e-07, 9.553449396548785e-07, 9.55257518550449e-07, 9.551999016821355e-07, 9.5508466586125e-07]

adam_train_acc = [0.90538335, 0.9579333, 0.97153336, 0.9783667, 0.98366666, 0.98728335, 0.99, 0.99291664, 0.9946333, 0.9960833, 0.9967, 0.9979, 0.99871665, 0.99881667, 0.99895, 0.99946666, 0.9996667, 0.9987, 0.9997333, 0.9996167, 0.99838334, 0.9999667, 0.99993336, 0.99965, 1.0, 1.0, 1.0, 0.99768335, 0.9999833, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9989, 0.99848336, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9981833, 0.99895, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99745, 0.99981666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99986666, 0.9975, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99755, 0.9999, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99765, 0.9996167, 0.99993336, 0.9999667, 0.99955, 0.99915, 0.99978334, 0.99995, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]



# plot(adam_train_acc)
# plot(adam_097decayLoss)

# plot(adam_097decayLoss,'loss',adam_097decayAccuracy,'accuracy')


#plot(tdecay001,'0.001 decay every iteration',nodecay_50,'no decay')



#
#
#
# plt.plot(dim,[0.9168, 0.9274, 0.9339, 0.9399, 0.9439, 0.9465, 0.9485, 0.9499, 0.9509, 0.9469], 'bo--')
# plt.plot(dim,[0.9184, 0.9302, 0.9378, 0.9443, 0.9468, 0.9466, 0.9491, 0.9504, 0.9532, 0.9535], 'bo--',label='Scaling')
# plt.xticks(dim)
# plt.title('Effect of Scaling')
# plt.legend(loc='lower right')
# plt.xlabel("Epoch")
# plt.ylabel("Accuracy")
# plt.show()
#
# t = np.arange(0., 11)

# loss
train_loss = [0.18350566323225698, 0.07257228127447889, 0.04755054318831147, 0.035703154472673, 0.02989167019884917, 0.024702708049687015, 0.01970851257764783, 0.018988947405305226, 0.016259157798309994, 0.012515683802903368, 0.014616958233737022, 0.015108489812902614, 0.011416673745363823, 0.011584911062865709, 0.012306845029891822, 0.012893455747303809, 0.008836466566871573, 0.012646002847067166, 0.010125817780350795, 0.00832788477444827, 0.007457867318188391, 0.009839295224385107, 0.0070620328476879496, 0.012136062142165354, 0.007300042510519802, 0.006595793264531078, 0.009197384603876344, 0.00617431342163646, 0.007319714973972964, 0.01035129154938261, 0.008327834556863915, 0.00931563874648911, 0.0032494432077896816, 0.005621546243797108, 0.011633035879372736, 0.007068519499506939, 0.007952273571702903, 0.004319523916223223, 0.006524845056423866, 0.01233445390406151, 0.004697921691280555, 0.008048016782293626, 0.00782550661017126, 0.0009142127438158809, 0.00011852586767474804, 4.636638233298148e-05, 0.0008009334397072128, 0.0288975320256273, 0.008324218898561924, 0.007133829166562767, 0.007343906887250758, 0.004099296686140122, 0.0015601838218656401, 0.004999786514323622, 0.012395561295351172, 0.008770991453574576, 0.008480542323694636, 0.004558767280534634, 0.007348107573613921, 0.004296940530216868, 0.009188729075244548, 0.008575292401525682, 0.003732035156411181, 0.0059292765003309, 0.006893303767572642, 0.007585454480838128, 0.0027369853793406188, 0.004785454926520458, 0.00642210659492387, 0.012594874624282775, 0.003286364277464126, 0.005681763510295544, 0.002803212802550471, 0.00789223395857969, 0.010159079730733444, 0.004668068204239262, 0.006625848501045558, 0.008327373601722267, 0.007541986460683651, 0.0061237491568651116, 0.005068395255779497, 0.002346246911691689, 0.006300425956741454, 0.009224118875090805, 0.008533906552732352, 0.0056949065672936665, 0.005499573788042369, 0.011889436488315539, 0.007581031220214053, 0.007662344424504587, 0.005096156849910377, 0.006402827664930726, 0.005878405423157081, 0.008977929431189295, 0.003994465034707939, 0.004971289051053608, 0.0059270732673076055, 0.014680188079889547, 0.008852552835670338, 0.007938096533601708, 0.005657324187214669, 0.01419572950612519, 0.00782261877784208, 0.00464834572893371, 0.005958360476396649, 0.007540201599680454, 0.008713134936392824, 0.010383432374317275, 0.007487278773045792, 0.0053901628770051955, 0.007877477215326868, 0.009614950023378413, 0.0066120624610443695, 0.005334461523810035, 0.006319847689850917, 0.007422424246407975, 0.008210926320784514, 0.008910921564714064, 0.012961470858863984, 0.006375025059893365, 0.0059918274752993736, 0.0063811604953745394, 0.011535295780897173, 0.010793444530020888, 0.008878720229790625, 0.01105807890257457, 0.009081246963548135, 0.010668256539707575, 0.010133291296935455, 0.007377253779518564, 0.007373398552800078, 0.0070352576219427, 0.0030766550710715516, 0.009860470201407642, 0.009595474680058374, 0.008696861741237663, 0.0070613064808321725, 0.009992459310116905, 0.009945347598706841, 0.01565816930399175, 0.012592190325476243, 0.005980015613561136, 0.006539077521515158, 0.007101980340201427, 0.008730181281113119, 0.011566522279557224, 0.00808433767515557, 0.011074858679693496, 0.011512327021557193, 0.012135164391402876, 0.008089154058085437, 0.010181848021038414, 0.011182628643910751, 0.00570729361413773, 0.007300284519852482, 0.011677248287079086, 0.005188881157565391, 0.009777536930375277, 0.008425309493594853, 0.013646294521515567, 0.014461074639400048, 0.014102689207335478, 0.011955298832687238, 0.013656828794949831, 0.009554974330847206, 0.0076497965660027485, 0.006129860164051593, 0.006205733633319142, 0.010309410517563152, 0.012686132725654697, 0.00945312742515038, 0.0071928058950188264, 0.012448523316332208, 0.006081669848051376, 0.007591504203422327, 0.007562938850815612, 0.005897952305410475, 0.010266293301770115, 0.019732008050062387, 0.011239784703516402, 0.010712042873931769, 0.00628719346489864, 0.006045508863280513, 0.014422951055647388, 0.019188803913522463, 0.013226052933599325, 0.011629066540584366, 0.013857132968953086, 0.01045870976818918, 0.015306322127899345, 0.01220764131334543, 0.010365265642295658, 0.01316467416867662, 0.010353392599443604, 0.011420397363672615, 0.014084304557822709, 0.012980658577231926, 0.011942767895301264, 0.009125608671433744, 0.009112407243917554]
test_loss = [0.10613723864778876, 0.06674972214270383, 0.06871544898487628, 0.08236572192516178, 0.07024052859982476, 0.09993712355848401, 0.0944524633535184, 0.08490572722512298, 0.08296191860048566, 0.08029481604447937, 0.09933254048111849, 0.09538660032609186, 0.08972202131597441, 0.09782644921913743, 0.1063213622677722, 0.08033141982377856, 0.09823178842074412, 0.09015161199295108, 0.09688156213571347, 0.12264591267357901, 0.12256437067619118, 0.11457463805239967, 0.12451661310944473, 0.10783851282009095, 0.11365635184978601, 0.12028416358152753, 0.12322273709035471, 0.11197827721793147, 0.126656896302311, 0.12697944632936242, 0.12192448844969477, 0.10463312191365617, 0.1040230238685399, 0.16141736836686846, 0.12369644945131995, 0.11738992783228241, 0.11139156580928102, 0.12595243711014065, 0.12394692316784131, 0.12246357522032782, 0.11896937331516255, 0.13108790729504563, 0.11460247482934051, 0.10747246514974904, 0.10639191876842916, 0.10279095162816247, 0.11187977044361105, 0.1477971065748079, 0.1256521110967651, 0.12555622626103458, 0.11861695360356407, 0.13071017872241897, 0.10857475382378994, 0.1406602785794587, 0.15080102269931445, 0.1418723570302626, 0.15927906462045258, 0.16853964452986248, 0.13482038362484106, 0.14582483813537692, 0.15557730117348853, 0.13193156131751266, 0.1421773946313624, 0.13371099692757185, 0.13043873079271293, 0.15240405726395012, 0.14403411908130237, 0.1505567543782888, 0.1947700366794743, 0.15090518621774607, 0.14966695043589198, 0.14692323078718686, 0.12356974401124944, 0.1775132241243341, 0.1616370799862841, 0.15062244312056577, 0.15318569971641693, 0.21119565365983703, 0.14605257709811326, 0.18306359089304464, 0.16095542461231502, 0.1599427055601683, 0.16098766839049575, 0.17435991669233147, 0.17725851200433454, 0.1714832734496639, 0.16661568596511528, 0.17253834769388335, 0.1807715572582788, 0.16626514853873914, 0.19518634176216323, 0.15789599422395725, 0.19221971402001203, 0.15983931365749526, 0.15248953438674562, 0.20082789742007207, 0.17901246874436821, 0.22228211587331542, 0.17715046761383804, 0.18472533991526519, 0.18126074970980938, 0.18246254708138282, 0.20256988271059015, 0.2066523890803347, 0.2013534805930692, 0.20874137549946395, 0.20318037682359374, 0.19613313116618883, 0.19915139328082035, 0.20076583668480225, 0.20120929211500255, 0.2172396053900036, 0.20753958358021746, 0.18109877265100807, 0.19787312160675696, 0.204618113470853, 0.18350265916545652, 0.22289164242399184, 0.18920378225008905, 0.19795040192723662, 0.1982675133885067, 0.1920502634183157, 0.20962454762226626, 0.1994294655469554, 0.22875350714150386, 0.2460119680516243, 0.2009897059710164, 0.2022135995684323, 0.22755201973126477, 0.1850886822395489, 0.21145419526736872, 0.20560718173215947, 0.20166822530777437, 0.19369306453951993, 0.21416555792493963, 0.2116778519118492, 0.20355694644804204, 0.2251507038915378, 0.2223653373783708, 0.21416306664820414, 0.21053410954947194, 0.19937421937295313, 0.2159446920822347, 0.19236319543307842, 0.21632051006314615, 0.20633100718579953, 0.21573038596751193, 0.24827509307498016, 0.25448301838927767, 0.22329220828445615, 0.20204234206381158, 0.23037155139853494, 0.2340215465333165, 0.2129978784575235, 0.2152487088709438, 0.21319552225173993, 0.24387190794387434, 0.20145239445584537, 0.2122268493015804, 0.23297366877132616, 0.1984895223393363, 0.22494146359691514, 0.2463355066143441, 0.23524537437954224, 0.256606492460519, 0.260886544575896, 0.2207196412414868, 0.22031936585741105, 0.24045385655892687, 0.21740302106389722, 0.24854422136344792, 0.20715836256172962, 0.21345167610894236, 0.2157913787227278, 0.24429635553552834, 0.25104706365009244, 0.24254135618617464, 0.2858439093188026, 0.28782001885681896, 0.2512469635207782, 0.25299752574577217, 0.2549481272188621, 0.2379903319344505, 0.23506455983876667, 0.2560764195355671, 0.24040898770544003, 0.25174107421370817, 0.23281270618684574, 0.23523653693429478, 0.2665719657956527, 0.2283800144961652, 0.24426620350043723, 0.2201392792384212, 0.23785558893934314, 0.22967085964813236, 0.2555575159096083, 0.24065572936492172, 0.26446755379067843, 0.2511734983840307, 0.23735436273998467]
train_accuracy = [0.94416666, 0.97721666, 0.98465, 0.98821664, 0.9900333, 0.99175, 0.99383336, 0.99406666, 0.9946833, 0.99598336, 0.9956333, 0.9952667, 0.99655, 0.99655, 0.99633336, 0.9961, 0.99735, 0.99635, 0.9971833, 0.99766666, 0.9977667, 0.99738336, 0.9981, 0.9969, 0.99806666, 0.99831665, 0.99765, 0.9985, 0.9981, 0.9974833, 0.9982333, 0.99773335, 0.99903333, 0.99873334, 0.9974333, 0.99825, 0.9981167, 0.99895, 0.9985833, 0.99756664, 0.9989833, 0.99775, 0.9983, 0.99965, 0.9999833, 0.9999667, 0.99988335, 0.99513334, 0.99838334, 0.99845, 0.99845, 0.99905, 0.99955, 0.99901664, 0.99763334, 0.9982333, 0.9986167, 0.99908334, 0.99871665, 0.99906665, 0.99835, 0.99831665, 0.99918336, 0.9988667, 0.99875, 0.99865, 0.99936664, 0.99911666, 0.99885, 0.99796665, 0.9992, 0.99916667, 0.99953336, 0.99878335, 0.9983, 0.9991, 0.9989, 0.9988, 0.99878335, 0.999, 0.9992333, 0.9996, 0.99908334, 0.99871665, 0.99876666, 0.9992333, 0.9993167, 0.99838334, 0.99901664, 0.9989333, 0.99915, 0.9992167, 0.9992667, 0.99875, 0.9992167, 0.9992833, 0.99916667, 0.99815, 0.9986333, 0.9989333, 0.9993, 0.9983, 0.9988833, 0.99916667, 0.99925, 0.99905, 0.9989333, 0.99871665, 0.99915, 0.99925, 0.9991, 0.9988, 0.99913335, 0.99945, 0.99916667, 0.99903333, 0.999, 0.9989167, 0.99855, 0.9992667, 0.9992833, 0.9992167, 0.9987, 0.99885, 0.99906665, 0.99876666, 0.9989667, 0.9989, 0.99876666, 0.99915, 0.99908334, 0.9992, 0.9995833, 0.99908334, 0.99908334, 0.99908334, 0.99916667, 0.99906665, 0.99915, 0.99843335, 0.99878335, 0.99941665, 0.9992667, 0.99915, 0.9991, 0.99885, 0.9992, 0.99883336, 0.9988833, 0.99878335, 0.99916667, 0.999, 0.99906665, 0.99943334, 0.9992833, 0.9989333, 0.99941665, 0.99913335, 0.9992167, 0.99876666, 0.99868333, 0.99878335, 0.9988, 0.99875, 0.99913335, 0.9992833, 0.99938333, 0.99946666, 0.99916667, 0.99883336, 0.99911666, 0.99935, 0.999, 0.99951667, 0.9992667, 0.99918336, 0.9995, 0.99906665, 0.99833333, 0.9989, 0.99915, 0.99948335, 0.99941665, 0.99876666, 0.99843335, 0.9988833, 0.9989833, 0.9989, 0.99916667, 0.99871665, 0.99905, 0.99915, 0.999, 0.99908334, 0.99903333, 0.99883336, 0.9989333, 0.99895, 0.9992, 0.9992333]
test_accuracy =  [0.9655, 0.9797, 0.9789, 0.9746, 0.982, 0.9755, 0.9783, 0.9804, 0.9797, 0.9818, 0.9763, 0.98, 0.9819, 0.9805, 0.9769, 0.9845, 0.9827, 0.9813, 0.9827, 0.9778, 0.9804, 0.9803, 0.9797, 0.9816, 0.982, 0.9821, 0.9808, 0.9827, 0.9802, 0.9819, 0.9822, 0.9833, 0.9849, 0.9783, 0.9807, 0.9829, 0.9832, 0.9821, 0.9827, 0.9831, 0.984, 0.9827, 0.9844, 0.9848, 0.9848, 0.986, 0.9831, 0.982, 0.9837, 0.9835, 0.9845, 0.9838, 0.9866, 0.9835, 0.982, 0.983, 0.9817, 0.9801, 0.9831, 0.9836, 0.9817, 0.9836, 0.9838, 0.9835, 0.9846, 0.9832, 0.984, 0.9829, 0.9794, 0.9838, 0.9855, 0.9843, 0.9863, 0.9817, 0.9841, 0.9847, 0.9846, 0.9796, 0.9848, 0.9823, 0.9843, 0.9843, 0.9834, 0.9829, 0.9826, 0.9832, 0.9836, 0.9829, 0.9834, 0.9843, 0.9805, 0.9846, 0.9825, 0.9839, 0.9849, 0.9805, 0.9842, 0.9806, 0.9841, 0.9827, 0.9834, 0.9844, 0.9816, 0.983, 0.9826, 0.9815, 0.9826, 0.9839, 0.9825, 0.9839, 0.9835, 0.9815, 0.983, 0.9849, 0.9834, 0.9829, 0.9845, 0.9821, 0.9828, 0.9838, 0.984, 0.9836, 0.984, 0.9845, 0.982, 0.9825, 0.9845, 0.9836, 0.9817, 0.9843, 0.9832, 0.9836, 0.984, 0.9843, 0.9825, 0.9837, 0.9838, 0.9826, 0.9829, 0.983, 0.9837, 0.9849, 0.9828, 0.9849, 0.9837, 0.9836, 0.9831, 0.981, 0.9812, 0.9835, 0.9842, 0.9834, 0.9833, 0.9835, 0.9837, 0.9845, 0.9826, 0.9857, 0.9848, 0.9824, 0.9852, 0.9827, 0.9826, 0.9832, 0.9814, 0.9814, 0.9845, 0.984, 0.9821, 0.9835, 0.9816, 0.984, 0.9838, 0.9839, 0.9825, 0.9822, 0.982, 0.9796, 0.9794, 0.9821, 0.9827, 0.9815, 0.9828, 0.9838, 0.9821, 0.9831, 0.9824, 0.9842, 0.9834, 0.9815, 0.984, 0.9827, 0.9848, 0.9827, 0.9836, 0.9825, 0.9833, 0.9814, 0.9824, 0.9829]


plot_4_history(train_loss,train_accuracy,test_loss,test_accuracy)

